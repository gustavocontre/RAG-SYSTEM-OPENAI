# Sistema RAG - Asistente de Conocimiento para Desarrolladores

Sistema completo de Retrieval-Augmented Generation (RAG) que procesa documentaciÃ³n tÃ©cnica, la indexa en una base de datos vectorial, y proporciona respuestas precisas a consultas utilizando LLMs. Implementado con FastAPI, Gradio, ChromaDB y OpenAI GPT-4.

## ğŸ“‹ Tabla de Contenidos

1. [CaracterÃ­sticas](#-caracterÃ­sticas)
2. [Arquitectura](#-arquitectura)
3. [Requisitos Previos](#-requisitos-previos)
4. [InstalaciÃ³n Paso a Paso](#-instalaciÃ³n-paso-a-paso)
5. [DocumentaciÃ³n de APIs](#-documentaciÃ³n-de-apis)
6. [Stack TecnolÃ³gico](#-stack-tecnolÃ³gico)
7. [Decisiones ArquitectÃ³nicas](#-decisiones-arquitectÃ³nicas)
8. [Estrategias de EvaluaciÃ³n](#-estrategias-de-evaluaciÃ³n)
9. [Uso del Sistema](#-uso-del-sistema)
10. [SoluciÃ³n de Problemas](#-soluciÃ³n-de-problemas)

## ğŸ¯ CaracterÃ­sticas

- âœ… **Procesamiento de Documentos**: Soporta PDF, TXT, MD con chunking inteligente
- âœ… **BÃºsqueda SemÃ¡ntica**: Embeddings con Sentence-Transformers y ChromaDB
- âœ… **GeneraciÃ³n de Respuestas**: IntegraciÃ³n con OpenAI GPT-4-turbo
- âœ… **Interfaz Web**: Chat interactivo con Gradio
- âœ… **API REST**: Endpoints completos con documentaciÃ³n Swagger
- âœ… **MÃ©tricas y EvaluaciÃ³n**: Sistema de tracking y evaluaciÃ³n de rendimiento
- âœ… **Docker**: ContenerizaciÃ³n completa con Docker Compose
- âœ… **ConfiguraciÃ³n Segura**: Manejo de secrets y variables de entorno

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Interfaz Web (Gradio)                    â”‚
â”‚                    Puerto: 7860                            â”‚
â”‚  - Chat interactivo                                         â”‚
â”‚  - Subida de documentos                                     â”‚
â”‚  - VisualizaciÃ³n de fuentes                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ HTTP REST
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API REST (FastAPI)                        â”‚
â”‚                    Puerto: 8000                             â”‚
â”‚  - /upload: Procesamiento de documentos                     â”‚
â”‚  - /query: Consultas RAG                                    â”‚
â”‚  - /stats: EstadÃ­sticas                                     â”‚
â”‚  - /metrics: MÃ©tricas del sistema                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                â”‚
            â–¼                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    ChromaDB          â”‚      â”‚    OpenAI GPT-4               â”‚
â”‚  (Base Vectorial)    â”‚      â”‚    (LLM)                      â”‚
â”‚                      â”‚      â”‚                               â”‚
â”‚  - Embeddings        â”‚      â”‚  - GeneraciÃ³n de respuestas   â”‚
â”‚  - BÃºsqueda semÃ¡nticaâ”‚      â”‚  - Contexto aumentado        â”‚
â”‚  - Metadata filteringâ”‚      â”‚  - Zero-shot learning         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo de Datos

1. **Ingesta de Documentos**:
   ```
   PDF/TXT/MD â†’ ExtracciÃ³n de texto â†’ Chunking â†’ Embeddings â†’ ChromaDB
   ```

2. **Consulta RAG**:
   ```
   Pregunta â†’ Embedding â†’ BÃºsqueda semÃ¡ntica â†’ Chunks relevantes â†’ 
   Contexto â†’ GPT-4 â†’ Respuesta + Fuentes
   ```

### Componentes Principales

- **DocumentProcessor**: Extrae texto, genera chunks, crea embeddings
- **RAGQueryService**: BÃºsqueda semÃ¡ntica y generaciÃ³n de respuestas
- **API REST**: FastAPI con endpoints RESTful
- **Interfaz Web**: Gradio para interacciÃ³n con usuarios
- **MetricsCollector**: Sistema de mÃ©tricas y evaluaciÃ³n

## ğŸ“‹ Requisitos Previos

### Software Requerido

- **Python 3.9 o superior**
- **pip** (gestor de paquetes Python)
- **Git** (para clonar el repositorio)

### Opcional (para Docker)

- **Docker Desktop** (versiÃ³n 20.10+)
- **Docker Compose** (versiÃ³n 2.0+)

### Credenciales

- **API Key de OpenAI**: Obtener en https://platform.openai.com/api-keys
  - Requiere cuenta en OpenAI
  - Costo asociado por uso (ver precios en OpenAI)

## ğŸš€ InstalaciÃ³n Paso a Paso

### OpciÃ³n 1: InstalaciÃ³n Local (Recomendado para Desarrollo)

#### Paso 1: Clonar el Repositorio

```bash
git clone https://github.com/TU_USUARIO/rag-system-openai.git
cd rag-system-openai
```

#### Paso 2: Crear Entorno Virtual (Recomendado)

**Windows:**
```bash
python -m venv venv
venv\Scripts\activate
```

**Linux/Mac:**
```bash
python3 -m venv venv
source venv/bin/activate
```

#### Paso 3: Instalar Dependencias

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

**Tiempo estimado**: 2-5 minutos (depende de la conexiÃ³n)

#### Paso 4: Configurar Variables de Entorno

**MÃ©todo RÃ¡pido (Recomendado):**
```bash
python scripts/setup_env.py
```

Este script te guiarÃ¡ interactivamente para:
- Configurar tu API key de OpenAI
- Ajustar puertos y configuraciones
- Validar la configuraciÃ³n

**MÃ©todo Manual:**
```bash
# 1. Copiar archivo de ejemplo
cp env.example .env

# 2. Editar .env con tu editor favorito
# Windows: notepad .env
# Linux/Mac: nano .env o vim .env

# 3. Configurar tu API key
OPENAI_API_KEY=sk-tu_api_key_real_aqui
```

**Verificar configuraciÃ³n:**
```bash
python scripts/check_config.py
```

#### Paso 5: Iniciar el Sistema

**Terminal 1 - Servidor API:**
```bash
python -m services.web_interface.api
```

DeberÃ­as ver:
```
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     Started server process
INFO:     Application startup complete.
```

**Terminal 2 - Interfaz Web:**
```bash
python -m services.web_interface.gradio_ui
```

DeberÃ­as ver:
```
Running on local URL:  http://127.0.0.1:7860
```

#### Paso 6: Verificar que Funciona

1. **Health Check:**
   ```bash
   curl http://localhost:8000/health
   ```
   O abre en navegador: http://localhost:8000/health

2. **DocumentaciÃ³n API:**
   Abre en navegador: http://localhost:8000/docs

3. **Interfaz Web:**
   Abre en navegador: http://localhost:7860

### OpciÃ³n 2: Docker (Recomendado para ProducciÃ³n)

#### Paso 1: Clonar y Configurar

```bash
git clone https://github.com/TU_USUARIO/rag-system-openai.git
cd rag-system-openai
cp env.example .env
# Editar .env con tu OPENAI_API_KEY
```

#### Paso 2: Construir y Iniciar

```bash
docker-compose up -d --build
```

Este comando:
- Construye las imÃ¡genes Docker
- Crea los contenedores
- Inicia los servicios en segundo plano

**Ver logs:**
```bash
docker-compose logs -f
```

#### Paso 3: Verificar

```bash
# Verificar contenedores
docker-compose ps

# DeberÃ­as ver:
# rag-api   Up   0.0.0.0:8000->8000/tcp
# rag-ui    Up   0.0.0.0:7860->7860/tcp
```

#### Paso 4: Detener Servicios

```bash
docker-compose down
```

## ğŸ“¡ DocumentaciÃ³n de APIs

### Base URL

```
http://localhost:8000
```

### Endpoints Principales

#### 1. Health Check

Verifica el estado del servicio y la base de datos.

**Endpoint:** `GET /health`

**Ejemplo de uso:**
```bash
curl http://localhost:8000/health
```

**Respuesta:**
```json
{
  "status": "healthy",
  "database": {
    "connected": true,
    "chunks": 718,
    "documents": 6
  }
}
```

---

#### 2. Subir Documento

Procesa y indexa un documento en la base de datos vectorial.

**Endpoint:** `POST /upload`

**Content-Type:** `multipart/form-data`

**ParÃ¡metros:**
- `file` (form-data, requerido): Archivo PDF, TXT o MD

**Ejemplo de uso con curl:**
```bash
curl -X POST "http://localhost:8000/upload" \
  -F "file=@documento.pdf"
```

**Ejemplo con Python:**
```python
import requests

url = "http://localhost:8000/upload"
files = {"file": open("documento.pdf", "rb")}
response = requests.post(url, files=files)
print(response.json())
```

**Respuesta exitosa:**
```json
{
  "doc_id": "doc_5b89c441ba45",
  "chunks_created": 120,
  "total_chars": 45678,
  "filename": "documento.pdf",
  "message": "Documento procesado exitosamente"
}
```

**Errores comunes:**
- `400`: Formato no soportado
- `500`: Error al procesar el documento

---

#### 3. Realizar Consulta RAG

Procesa una consulta y genera una respuesta usando RAG.

**Endpoint:** `POST /query`

**Content-Type:** `application/json`

**Body:**
```json
{
  "question": "Â¿QuÃ© es Python?",
  "top_k": 5,
  "return_sources": true
}
```

**ParÃ¡metros:**
- `question` (string, requerido): Pregunta del usuario
- `top_k` (int, opcional): NÃºmero de chunks a recuperar (default: 5)
- `return_sources` (bool, opcional): Si retornar fuentes (default: true)

**Ejemplo de uso con curl:**
```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Â¿QuÃ© es Python?",
    "top_k": 5,
    "return_sources": true
  }'
```

**Ejemplo con Python:**
```python
import requests

url = "http://localhost:8000/query"
payload = {
    "question": "Â¿CÃ³mo definir una funciÃ³n en Python?",
    "top_k": 5,
    "return_sources": True
}
response = requests.post(url, json=payload)
result = response.json()

print("Respuesta:", result["answer"])
print("Fuentes:", result["sources"])
```

**Respuesta:**
```json
{
  "answer": "Python es un lenguaje de programaciÃ³n de alto nivel...",
  "sources": [
    {
      "filename": "Python para todos.pdf",
      "chunk_index": 2,
      "score": 0.85
    },
    {
      "filename": "Introduccion a Python.pdf",
      "chunk_index": 5,
      "score": 0.78
    }
  ],
  "num_chunks": 5
}
```

**Proceso interno:**
1. Genera embedding de la pregunta
2. BÃºsqueda semÃ¡ntica en ChromaDB
3. Recupera top-k chunks mÃ¡s relevantes
4. Genera respuesta con GPT-4 usando contexto
5. Retorna respuesta con fuentes

---

#### 4. Listar Documentos

Obtiene la lista de documentos indexados.

**Endpoint:** `GET /documents`

**Ejemplo:**
```bash
curl http://localhost:8000/documents
```

**Respuesta:**
```json
{
  "documents": [
    "doc_5b89c441ba45",
    "doc_7c3d2e1f9a8b"
  ],
  "count": 2
}
```

---

#### 5. EstadÃ­sticas

Obtiene estadÃ­sticas de la base de datos.

**Endpoint:** `GET /stats`

**Ejemplo:**
```bash
curl http://localhost:8000/stats
```

**Respuesta:**
```json
{
  "total_chunks": 718,
  "unique_documents": 6
}
```

---

#### 6. MÃ©tricas del Sistema

Obtiene mÃ©tricas de rendimiento.

**Endpoint:** `GET /metrics`

**Ejemplo:**
```bash
curl http://localhost:8000/metrics
```

**Respuesta:**
```json
{
  "timestamp": "2025-11-04T18:00:00",
  "system_stats": {
    "total_chunks": 718,
    "unique_documents": 6,
    "db_size_mb": 19.52
  },
  "query_metrics": {
    "total_queries": 25,
    "time_metrics": {
      "total_time": {
        "mean": 2.5,
        "min": 1.2,
        "max": 5.8
      }
    }
  }
}
```

---

#### 7. Eliminar Documento

Elimina un documento y todos sus chunks.

**Endpoint:** `DELETE /delete/{doc_id}`

**Ejemplo:**
```bash
curl -X DELETE "http://localhost:8000/delete/doc_5b89c441ba45"
```

---

### DocumentaciÃ³n Interactiva

Accede a la documentaciÃ³n interactiva (Swagger) en:
```
http://localhost:8000/docs
```

O la documentaciÃ³n alternativa (ReDoc) en:
```
http://localhost:8000/redoc
```

### Ejemplo Completo: Flujo de Trabajo

```bash
# 1. Verificar estado
curl http://localhost:8000/health

# 2. Subir documento
curl -X POST "http://localhost:8000/upload" \
  -F "file=@python_tutorial.pdf"

# 3. Ver estadÃ­sticas
curl http://localhost:8000/stats

# 4. Hacer consulta
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Â¿CÃ³mo definir una funciÃ³n en Python?",
    "top_k": 5
  }'

# 5. Ver mÃ©tricas
curl http://localhost:8000/metrics
```

## ğŸ”§ Stack TecnolÃ³gico

### Backend

| TecnologÃ­a | VersiÃ³n | PropÃ³sito |
|------------|---------|-----------|
| **Python** | 3.9+ | Lenguaje principal del sistema |
| **FastAPI** | 0.104+ | Framework web moderno y rÃ¡pido para API REST |
| **Uvicorn** | 0.24+ | Servidor ASGI de alto rendimiento |
| **LangChain** | 0.1+ | Framework para aplicaciones LLM |
| **ChromaDB** | 0.4+ | Base de datos vectorial para embeddings |
| **Sentence-Transformers** | 2.2+ | Modelo de embeddings semÃ¡nticos |
| **OpenAI** | 1.0+ | SDK para integraciÃ³n con GPT-4 |
| **PyPDF2** | 3.0+ | ExtracciÃ³n de texto de PDFs |
| **python-dotenv** | 1.0+ | Manejo de variables de entorno |

### Frontend

| TecnologÃ­a | VersiÃ³n | PropÃ³sito |
|------------|---------|-----------|
| **Gradio** | 4.0+ | Interfaz web interactiva para ML/AI |
| **HTML/CSS/JS** | - | Interfaz de usuario (generada por Gradio) |

### Infraestructura

| TecnologÃ­a | VersiÃ³n | PropÃ³sito |
|------------|---------|-----------|
| **Docker** | 20.10+ | ContenerizaciÃ³n de aplicaciones |
| **Docker Compose** | 2.0+ | OrquestaciÃ³n de servicios |

### Herramientas de Desarrollo

| TecnologÃ­a | PropÃ³sito |
|------------|-----------|
| **Git** | Control de versiones |
| **pydantic** | ValidaciÃ³n de datos |
| **pytest** | Testing (opcional) |

## ğŸ›ï¸ Decisiones ArquitectÃ³nicas

### 1. Base de Datos Vectorial: ChromaDB

**DecisiÃ³n**: Usar ChromaDB como base de datos vectorial principal.

**Razones:**
- **Simplicidad**: FÃ¡cil de instalar y configurar (no requiere servidor externo)
- **Persistencia local**: Almacena datos localmente sin dependencias externas
- **Rendimiento**: Excelente para bases pequeÃ±as-medianas (< 100K vectores)
- **Metadata filtering**: Soporte nativo para filtrar por metadata
- **Open source**: CÃ³digo abierto y activamente mantenido

**Alternativas consideradas:**
- **Milvus**: MÃ¡s complejo, requiere servidor separado
- **Pinecone**: Servicio cloud, requiere suscripciÃ³n
- **Weaviate**: MÃ¡s pesado, overkill para este caso

**Trade-offs aceptados:**
- Limitado a bases de datos locales
- Rendimiento puede degradarse con > 1M vectores

### 2. Modelo de Embeddings: Sentence-Transformers (all-MiniLM-L6-v2)

**DecisiÃ³n**: Usar modelo local de Sentence-Transformers.

**Razones:**
- **Sin dependencias externas**: No requiere API keys adicionales
- **Velocidad**: Modelo optimizado para rapidez (6 layers, 384 dimensions)
- **Calidad**: Buen balance calidad/velocidad para espaÃ±ol
- **Costo**: Gratis, sin costos por embedding
- **Privacidad**: Datos no salen del servidor

**Alternativas consideradas:**
- **OpenAI Embeddings**: Mayor calidad pero con costo y latencia
- **Cohere**: Similar a OpenAI, requiere API key
- **Modelos mÃ¡s grandes**: Mejor calidad pero mÃ¡s lentos

**Trade-offs aceptados:**
- Calidad ligeramente inferior a embeddings de OpenAI
- Requiere descarga inicial del modelo (~90MB)

### 3. LLM: OpenAI GPT-4-turbo

**DecisiÃ³n**: Usar GPT-4-turbo como modelo de generaciÃ³n.

**Razones:**
- **Calidad superior**: Mejor entendimiento de contexto y generaciÃ³n
- **API estable**: Servicio confiable y bien documentado
- **Capacidades avanzadas**: Buen manejo de instrucciones complejas
- **ActualizaciÃ³n automÃ¡tica**: Siempre Ãºltima versiÃ³n sin reentrenamiento

**Alternativas consideradas:**
- **GPT-3.5-turbo**: MÃ¡s rÃ¡pido y barato, pero menor calidad
- **Anthropic Claude**: Similar calidad, pero menos integrado
- **Groq**: Muy rÃ¡pido pero requiere modelo especÃ­fico
- **Modelos locales (Llama)**: Sin costo pero requiere GPU potente

**Trade-offs aceptados:**
- Costo por token (aproximadamente $0.01-0.03 por consulta)
- Dependencia de conexiÃ³n a internet
- Latencia de red (~2-5 segundos por consulta)

### 4. Estrategia de Chunking

**DecisiÃ³n**: Chunking por palabras con overlap.

**ConfiguraciÃ³n:**
- `CHUNK_SIZE=500` palabras
- `CHUNK_OVERLAP=50` palabras

**Razones:**
- **Preserva contexto**: Overlap evita perder informaciÃ³n en lÃ­mites
- **TamaÃ±o Ã³ptimo**: 500 palabras balancea contexto y precisiÃ³n
- **Por palabras, no caracteres**: Mejor calidad semÃ¡ntica
- **Configurable**: FÃ¡cil ajustar segÃºn necesidad

**Alternativas consideradas:**
- **Chunking por caracteres**: MÃ¡s simple pero peor calidad
- **Chunking por pÃ¡rrafos**: MÃ¡s natural pero tamaÃ±o variable
- **Chunking inteligente (semÃ¡ntico)**: Mejor pero mÃ¡s complejo

**Trade-offs aceptados:**
- Puede dividir conceptos entre chunks
- Overlap aumenta tamaÃ±o de la BD

### 5. Framework Web: FastAPI

**DecisiÃ³n**: Usar FastAPI para la API REST.

**Razones:**
- **Rendimiento**: Muy rÃ¡pido (comparable a Node.js)
- **DocumentaciÃ³n automÃ¡tica**: Swagger/OpenAPI integrado
- **Type hints**: ValidaciÃ³n automÃ¡tica con Pydantic
- **Async/await**: Soporte nativo para operaciones asÃ­ncronas
- **Moderno**: DiseÃ±o limpio y Pythonic

**Alternativas consideradas:**
- **Flask**: MÃ¡s simple pero menos features
- **Django**: MÃ¡s pesado, overkill para API
- **Express.js**: RequerirÃ­a cambiar stack

### 6. Interfaz Web: Gradio

**DecisiÃ³n**: Usar Gradio para la interfaz de usuario.

**Razones:**
- **RÃ¡pido de desarrollar**: Interfaz lista en minutos
- **Interactivo**: Chat, uploads, visualizaciÃ³n incluidos
- **Sin frontend**: No requiere HTML/CSS/JS manual
- **IntegraciÃ³n fÃ¡cil**: Se conecta directamente a la API
- **Gratis y open source**: Sin restricciones

**Alternativas consideradas:**
- **Streamlit**: Similar pero menos flexible
- **React/Vue**: MÃ¡s control pero mucho mÃ¡s trabajo
- **HTML/CSS/JS puro**: MÃ¡ximo control pero desarrollo largo

### 7. Arquitectura de Servicios

**DecisiÃ³n**: Separar servicios en mÃ³dulos independientes.

**Estructura:**
```
services/
â”œâ”€â”€ document_processor/  # Procesamiento
â”œâ”€â”€ rag_query/          # Consultas RAG
â”œâ”€â”€ web_interface/      # API y UI
â””â”€â”€ metrics/            # MÃ©tricas
```

**Razones:**
- **SeparaciÃ³n de responsabilidades**: Cada mÃ³dulo tiene una funciÃ³n clara
- **Reutilizable**: MÃ³dulos pueden usarse independientemente
- **Testeable**: FÃ¡cil testear cada componente
- **Escalable**: FÃ¡cil agregar nuevos servicios

### 8. Manejo de ConfiguraciÃ³n

**DecisiÃ³n**: Sistema centralizado de configuraciÃ³n con `config/settings.py`.

**Razones:**
- **Un solo punto de verdad**: Toda la configuraciÃ³n en un lugar
- **ValidaciÃ³n automÃ¡tica**: Verifica configuraciÃ³n al inicio
- **Type-safe**: Type hints para todas las variables
- **Seguridad**: Manejo seguro de secrets

## ğŸ“Š Estrategias de EvaluaciÃ³n

### 1. MÃ©tricas Implementadas

#### MÃ©tricas de RecuperaciÃ³n (Retrieval)

**Score de Similitud (Cosine Similarity)**
- **QuÃ© mide**: Relevancia de chunks recuperados
- **Rango**: 0.0 - 1.0 (1.0 = perfectamente relevante)
- **Umbral recomendado**: > 0.5 para chunks Ãºtiles
- **ImplementaciÃ³n**: Calculado automÃ¡ticamente en cada bÃºsqueda

**Precision@K**
- **QuÃ© mide**: Porcentaje de chunks relevantes en los top-K
- **CÃ¡lculo**: `chunks_relevantes / K`
- **Objetivo**: > 70% para K=5

**Tiempo de RecuperaciÃ³n**
- **QuÃ© mide**: Velocidad de bÃºsqueda semÃ¡ntica
- **Objetivo**: < 500ms para bases < 10K chunks
- **Implementado**: Tracking automÃ¡tico

#### MÃ©tricas de GeneraciÃ³n

**Tiempo de GeneraciÃ³n**
- **QuÃ© mide**: Latencia del LLM
- **Depende de**: Modelo, contexto, longitud de respuesta
- **Objetivo**: 2-5 segundos con GPT-4

**Longitud de Respuesta**
- **QuÃ© mide**: Completitud de la respuesta
- **AnÃ¡lisis**: Respuestas muy cortas (< 50 chars) pueden indicar falta de contexto

**Relevancia de Respuesta**
- **QuÃ© mide**: Si la respuesta responde a la pregunta
- **EvaluaciÃ³n**: Manual o con LLM evaluador
- **MÃ©trica**: 0-5 (subjetiva)

#### MÃ©tricas del Sistema

**Throughput**
- **QuÃ© mide**: Consultas procesadas por segundo
- **CÃ¡lculo**: `total_queries / total_time`
- **Objetivo**: 0.2-0.5 queries/segundo con GPT-4

**Tasa de Ã‰xito**
- **QuÃ© mide**: Porcentaje de consultas exitosas
- **Objetivo**: > 95%

### 2. Herramientas de EvaluaciÃ³n

#### Script de MÃ©tricas

```bash
python scripts/generate_metrics_report.py
```

Genera reporte con:
- Tiempos promedio, mÃ­nimos, mÃ¡ximos
- Scores de similitud
- EstadÃ­sticas de uso
- Ãšltimas 10 consultas

#### API de MÃ©tricas

```bash
curl http://localhost:8000/metrics
```

Retorna mÃ©tricas en tiempo real en formato JSON.

### 3. MÃ©todos de EvaluaciÃ³n

#### EvaluaciÃ³n AutomÃ¡tica

**MÃ©tricas cuantitativas**:
- Tiempos de respuesta
- Scores de similitud
- Throughput
- Tasa de errores

**ImplementaciÃ³n**:
- Tracking automÃ¡tico en cada consulta
- Almacenamiento en `data/metrics.json`
- Reportes generados automÃ¡ticamente

#### EvaluaciÃ³n Manual

**Checklist de calidad** (para cada respuesta):
- [ ] Relevancia (0-5): Â¿Responde a la pregunta?
- [ ] PrecisiÃ³n (0-5): Â¿La informaciÃ³n es correcta?
- [ ] Completitud (0-5): Â¿EstÃ¡ completa la respuesta?
- [ ] Coherencia (0-5): Â¿Tiene sentido?

**Dataset de prueba**:
- Crear conjunto de preguntas con respuestas esperadas
- Comparar respuestas generadas vs esperadas
- Calcular mÃ©tricas de precisiÃ³n/recall

### 4. Benchmarking

#### MÃ©tricas de Referencia

Para un sistema RAG con:
- Base de datos: 1000-5000 chunks
- Modelo: GPT-4-turbo
- Embeddings: Sentence-Transformers

**MÃ©tricas esperadas**:
- Tiempo de recuperaciÃ³n: 200-500ms
- Tiempo de generaciÃ³n: 2-5s
- Tiempo total: 2.5-6s
- Score promedio: 0.6-0.8
- Throughput: 0.15-0.4 queries/segundo

#### ComparaciÃ³n con Baselines

**Baseline 1: Sin RAG (solo LLM)**
- Sin contexto de documentos
- Respuestas genÃ©ricas
- No especÃ­ficas al dataset

**Baseline 2: RAG con bÃºsqueda exacta**
- BÃºsqueda por palabras clave
- Sin embeddings
- Menor precisiÃ³n semÃ¡ntica

**Baseline 3: Diferente modelo de embeddings**
- Comparar Sentence-Transformers vs OpenAI embeddings
- Trade-off calidad/velocidad

### 5. OptimizaciÃ³n Basada en MÃ©tricas

**Si score de similitud < 0.5**:
- Ajustar `CHUNK_SIZE`
- Cambiar modelo de embeddings
- Mejorar preprocesamiento

**Si tiempo de respuesta > 10s**:
- Reducir `TOP_K_RESULTS`
- Usar GPT-3.5 en lugar de GPT-4
- Optimizar bÃºsqueda vectorial

**Si respuestas incompletas**:
- Aumentar `TOP_K_RESULTS`
- Aumentar `CHUNK_SIZE`
- Mejorar prompt del LLM

### 6. Monitoreo Continuo

**MÃ©tricas a monitorear**:
1. Latencia por consulta
2. Throughput (consultas/minuto)
3. Score promedio de similitud
4. Tasa de errores
5. Uso de tokens de OpenAI

**Alertas recomendadas**:
- Tiempo de respuesta > 10s
- Score promedio < 0.4
- Tasa de errores > 5%

Para mÃ¡s detalles, ver [EVALUACION_RAG.md](EVALUACION_RAG.md)

## ğŸ’» Uso del Sistema

### Flujo de Trabajo TÃ­pico

1. **Subir Documentos**
   ```bash
   curl -X POST "http://localhost:8000/upload" \
     -F "file=@documento.pdf"
   ```

2. **Hacer Consultas**
   ```bash
   curl -X POST "http://localhost:8000/query" \
     -H "Content-Type: application/json" \
     -d '{"question": "Â¿QuÃ© es Python?"}'
   ```

3. **Ver MÃ©tricas**
   ```bash
   python scripts/generate_metrics_report.py
   ```

### Desde la Interfaz Web

1. Abre http://localhost:7860
2. Sube un documento PDF/TXT/MD
3. Haz preguntas en el chat
4. El sistema responderÃ¡ con informaciÃ³n de los documentos

## ğŸ› SoluciÃ³n de Problemas

### Error: "OPENAI_API_KEY no estÃ¡ configurada"

**SoluciÃ³n:**
1. Verifica que `.env` existe: `ls .env` o `dir .env`
2. Verifica contenido: `cat .env | grep OPENAI_API_KEY`
3. Debe contener: `OPENAI_API_KEY=sk-...`
4. Ejecuta: `python scripts/check_config.py`

### Error: "Base de datos vacÃ­a"

**SoluciÃ³n:**
1. Sube al menos un documento primero
2. Usa endpoint `/upload` o interfaz web
3. Verifica con: `curl http://localhost:8000/stats`

### Error: "Puerto en uso"

**SoluciÃ³n:**
```bash
# Cambiar puerto en .env
API_PORT=8001
UI_PORT=7861
```

### Error: "Connection refused" en Docker

**SoluciÃ³n:**
```bash
# Verificar contenedores
docker-compose ps

# Ver logs
docker-compose logs -f

# Reiniciar
docker-compose restart
```

### Error: "Module not found"

**SoluciÃ³n:**
```bash
# Reinstalar dependencias
pip install -r requirements.txt

# Verificar entorno virtual activado
which python  # Linux/Mac
where python  # Windows
```

## ğŸ“ Estructura del Proyecto

```
RAG-system-openai/
â”œâ”€â”€ config/                      # ConfiguraciÃ³n centralizada
â”‚   â”œâ”€â”€ settings.py              # Manejo de secrets y variables
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ services/                    # Servicios principales
â”‚   â”œâ”€â”€ document_processor/      # Procesamiento de documentos
â”‚   â”‚   â”œâ”€â”€ processor.py         # LÃ³gica de chunking y embeddings
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ rag_query/               # Servicio de consultas RAG
â”‚   â”‚   â”œâ”€â”€ query_service.py     # BÃºsqueda semÃ¡ntica y generaciÃ³n
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ web_interface/           # API y UI
â”‚   â”‚   â”œâ”€â”€ api.py               # Endpoints FastAPI
â”‚   â”‚   â”œâ”€â”€ gradio_ui.py         # Interfaz web
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ metrics/                 # Sistema de mÃ©tricas
â”‚       â”œâ”€â”€ metrics_collector.py # Colector de mÃ©tricas
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ scripts/                     # Scripts utilitarios
â”‚   â”œâ”€â”€ setup_env.py             # ConfiguraciÃ³n interactiva
â”‚   â”œâ”€â”€ setup_env_auto.py        # ConfiguraciÃ³n automÃ¡tica
â”‚   â”œâ”€â”€ check_config.py          # Verificar configuraciÃ³n
â”‚   â”œâ”€â”€ generate_metrics_report.py # Generar reportes
â”‚   â””â”€â”€ test_setup.py            # Verificar dependencias
â”œâ”€â”€ docker/                      # Dockerfiles
â”‚   â”œâ”€â”€ Dockerfile.api           # Imagen para API
â”‚   â””â”€â”€ Dockerfile.ui            # Imagen para UI
â”œâ”€â”€ data/                        # Datos (gitignored)
â”‚   â”œâ”€â”€ chroma_db/              # Base de datos vectorial
â”‚   â”œâ”€â”€ uploaded_documents/      # Documentos subidos
â”‚   â””â”€â”€ metrics.json             # MÃ©tricas (gitignored)
â”œâ”€â”€ docker-compose.yml           # OrquestaciÃ³n Docker
â”œâ”€â”€ requirements.txt             # Dependencias Python
â”œâ”€â”€ env.example                  # Plantilla de configuraciÃ³n
â”œâ”€â”€ README.md                    # Este archivo
â”œâ”€â”€ API_DOCUMENTATION.md         # DocumentaciÃ³n detallada de APIs
â”œâ”€â”€ CONFIGURACION.md             # ConfiguraciÃ³n de secrets
â”œâ”€â”€ EVALUACION_RAG.md            # Estrategias de evaluaciÃ³n
â””â”€â”€ INICIAR_SISTEMA.md           # GuÃ­a de inicio
```

## ğŸ”’ Seguridad

- âœ… Archivo `.env` en `.gitignore` (no se sube al repositorio)
- âœ… Secrets nunca se exponen en logs
- âœ… ValidaciÃ³n de configuraciÃ³n al inicio
- âœ… Manejo seguro de variables de entorno

Ver [CONFIGURACION.md](CONFIGURACION.md) para mÃ¡s detalles.

## ğŸ“š DocumentaciÃ³n Adicional

- **[API_DOCUMENTATION.md](API_DOCUMENTATION.md)**: DocumentaciÃ³n completa de todos los endpoints
- **[CONFIGURACION.md](CONFIGURACION.md)**: ConfiguraciÃ³n detallada de secrets y variables
- **[EVALUACION_RAG.md](EVALUACION_RAG.md)**: Estrategias completas de evaluaciÃ³n
- **[INICIAR_SISTEMA.md](INICIAR_SISTEMA.md)**: GuÃ­a paso a paso para iniciar
- **[EVALUACION_REQUERIMIENTOS.md](EVALUACION_REQUERIMIENTOS.md)**: Checklist de requerimientos

## ğŸš§ PrÃ³ximas Mejoras

- [ ] AutenticaciÃ³n de usuarios
- [ ] Soporte para mÃ¡s formatos (DOCX, HTML)
- [ ] CachÃ© de respuestas frecuentes
- [ ] EvaluaciÃ³n automÃ¡tica con dataset de prueba
- [ ] Soporte para mÃºltiples bases de datos vectoriales
- [ ] Dashboard de mÃ©tricas en tiempo real
- [ ] API de streaming para respuestas
- [ ] Filtrado avanzado por metadata

## ğŸ“„ Licencia

Este proyecto es una prueba tÃ©cnica para evaluaciÃ³n.

---

**DocumentaciÃ³n Interactiva**: http://localhost:8000/docs  
**Interfaz Web**: http://localhost:7860  
**Repositorio**: https://github.com/TU_USUARIO/rag-system-openai
