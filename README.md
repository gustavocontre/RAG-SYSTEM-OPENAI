# Sistema RAG - Asistente de Conocimiento para Desarrolladores

Sistema completo de Retrieval-Augmented Generation (RAG) que procesa documentaci√≥n t√©cnica, la indexa en una base de datos vectorial, y proporciona respuestas precisas a consultas utilizando LLMs. Implementado con FastAPI, Gradio, ChromaDB y OpenAI GPT-4.

## Tabla de Contenidos

1. [Caracter√≠sticas](#-caracter√≠sticas)
2. [Arquitectura](#-arquitectura)
3. [Requisitos Previos](#-requisitos-previos)
4. [Instalaci√≥n Paso a Paso](#-instalaci√≥n-paso-a-paso)
5. [Documentaci√≥n de APIs](#-documentaci√≥n-de-apis)
6. [Stack Tecnol√≥gico](#-stack-tecnol√≥gico)
7. [Decisiones Arquitect√≥nicas](#-decisiones-arquitect√≥nicas)
8. [Estrategias de Evaluaci√≥n](#-estrategias-de-evaluaci√≥n)
9. [Uso del Sistema](#-uso-del-sistema)
10. [Soluci√≥n de Problemas](#-soluci√≥n-de-problemas)

## Caracter√≠sticas

- **Procesamiento de Documentos**: Soporta PDF, TXT, MD con chunking inteligente
- **B√∫squeda Sem√°ntica**: Embeddings con Sentence-Transformers y ChromaDB
- **Generaci√≥n de Respuestas**: Integraci√≥n con OpenAI GPT-4-turbo
- **Interfaz Web**: Chat interactivo con Gradio
- **API REST**: Endpoints completos con documentaci√≥n Swagger
- **M√©tricas y Evaluaci√≥n**: Sistema de tracking y evaluaci√≥n de rendimiento
- **Docker**: Contenerizaci√≥n completa con Docker Compose
- **Configuraci√≥n Segura**: Manejo de secrets y variables de entorno

## Arquitectura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Interfaz Web (Gradio)                    ‚îÇ
‚îÇ                    Puerto: 7860                            ‚îÇ
‚îÇ  - Chat interactivo                                         ‚îÇ
‚îÇ  - Subida de documentos                                     ‚îÇ
‚îÇ  - Visualizaci√≥n de fuentes                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ HTTP REST
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    API REST (FastAPI)                        ‚îÇ
‚îÇ                    Puerto: 8000                             ‚îÇ
‚îÇ  - /upload: Procesamiento de documentos                     ‚îÇ
‚îÇ  - /query: Consultas RAG                                    ‚îÇ
‚îÇ  - /stats: Estad√≠sticas                                     ‚îÇ
‚îÇ  - /metrics: M√©tricas del sistema                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                                ‚îÇ
            ‚ñº                                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    ChromaDB          ‚îÇ      ‚îÇ    OpenAI GPT-4               ‚îÇ
‚îÇ  (Base Vectorial)    ‚îÇ      ‚îÇ    (LLM)                      ‚îÇ
‚îÇ                      ‚îÇ      ‚îÇ                               ‚îÇ
‚îÇ  - Embeddings        ‚îÇ      ‚îÇ  - Generaci√≥n de respuestas   ‚îÇ
‚îÇ  - B√∫squeda sem√°ntica‚îÇ      ‚îÇ  - Contexto aumentado        ‚îÇ
‚îÇ  - Metadata filtering‚îÇ      ‚îÇ  - Zero-shot learning         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Flujo de Datos

1. **Ingesta de Documentos**:
   ```
   PDF/TXT/MD ‚Üí Extracci√≥n de texto ‚Üí Chunking ‚Üí Embeddings ‚Üí ChromaDB
   ```

2. **Consulta RAG**:
   ```
   Pregunta ‚Üí Embedding ‚Üí B√∫squeda sem√°ntica ‚Üí Chunks relevantes ‚Üí 
   Contexto ‚Üí GPT-4 ‚Üí Respuesta + Fuentes
   ```

### Componentes Principales

- **DocumentProcessor**: Extrae texto, genera chunks, crea embeddings
- **RAGQueryService**: B√∫squeda sem√°ntica y generaci√≥n de respuestas
- **API REST**: FastAPI con endpoints RESTful
- **Interfaz Web**: Gradio para interacci√≥n con usuarios
- **MetricsCollector**: Sistema de m√©tricas y evaluaci√≥n

## Requisitos Previos

### Software Requerido

- **Python 3.9 o superior**
- **pip** (gestor de paquetes Python)
- **Git** (para clonar el repositorio)

### Opcional (para Docker)

- **Docker Desktop** (versi√≥n 20.10+)
- **Docker Compose** (versi√≥n 2.0+)

### Credenciales

- **API Key de OpenAI**: Obtener en https://platform.openai.com/api-keys
  - Requiere cuenta en OpenAI
  - Costo asociado por uso (ver precios en OpenAI)

## Instalaci√≥n Paso a Paso

### Opci√≥n 1: Instalaci√≥n Local (Recomendado para Desarrollo)

#### Paso 1: Clonar el Repositorio

```bash
git clone https://github.com/TU_USUARIO/rag-system-openai.git
cd rag-system-openai
```

#### Paso 2: Crear Entorno Virtual (Recomendado)

**Windows:**
```bash
python -m venv venv
venv\Scripts\activate
```

**Linux/Mac:**
```bash
python3 -m venv venv
source venv/bin/activate
```

#### Paso 3: Instalar Dependencias

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

**Tiempo estimado**: 2-5 minutos (depende de la conexi√≥n)

#### Paso 4: Configurar Variables de Entorno

**M√©todo R√°pido (Recomendado):**
```bash
python scripts/setup_env.py
```

Este script te guiar√° interactivamente para:
- Configurar tu API key de OpenAI
- Ajustar puertos y configuraciones
- Validar la configuraci√≥n

**M√©todo Manual:**
```bash
# 1. Copiar archivo de ejemplo
cp env.example .env

# 2. Editar .env con tu editor favorito
# Windows: notepad .env
# Linux/Mac: nano .env o vim .env

# 3. Configurar tu API key
OPENAI_API_KEY=sk-tu_api_key_real_aqui
```

**Verificar configuraci√≥n:**
```bash
python scripts/check_config.py
```

#### Paso 5: Iniciar el Sistema

**Terminal 1 - Servidor API:**
```bash
python -m services.web_interface.api
```

Deber√≠as ver:
```
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     Started server process
INFO:     Application startup complete.
```

**Terminal 2 - Interfaz Web:**
```bash
python -m services.web_interface.gradio_ui
```

Deber√≠as ver:
```
Running on local URL:  http://127.0.0.1:7860
```

#### Paso 6: Verificar que Funciona

1. **Health Check:**
   ```bash
   curl http://localhost:8000/health
   ```
   O abre en navegador: http://localhost:8000/health

2. **Documentaci√≥n API:**
   Abre en navegador: http://localhost:8000/docs

3. **Interfaz Web:**
   Abre en navegador: http://localhost:7860

### Opci√≥n 2: Docker (Recomendado para Producci√≥n)

#### Paso 1: Clonar y Configurar

```bash
git clone https://github.com/TU_USUARIO/rag-system-openai.git
cd rag-system-openai
cp env.example .env
# Editar .env con tu OPENAI_API_KEY
```

#### Paso 2: Construir y Iniciar

```bash
docker-compose up -d --build
```

Este comando:
- Construye las im√°genes Docker
- Crea los contenedores
- Inicia los servicios en segundo plano

**Ver logs:**
```bash
docker-compose logs -f
```

#### Paso 3: Verificar

```bash
# Verificar contenedores
docker-compose ps

# Deber√≠as ver:
# rag-api   Up   0.0.0.0:8000->8000/tcp
# rag-ui    Up   0.0.0.0:7860->7860/tcp
```

#### Paso 4: Detener Servicios

```bash
docker-compose down
```

## Documentaci√≥n de APIs

### Base URL

```
http://localhost:8000
```

### Endpoints Principales

#### 1. Health Check

Verifica el estado del servicio y la base de datos.

**Endpoint:** `GET /health`

**Ejemplo de uso:**
```bash
curl http://localhost:8000/health
```

**Respuesta:**
```json
{
  "status": "healthy",
  "database": {
    "connected": true,
    "chunks": 718,
    "documents": 6
  }
}
```

---

#### 2. Subir Documento

Procesa y indexa un documento en la base de datos vectorial.

**Endpoint:** `POST /upload`

**Content-Type:** `multipart/form-data`

**Par√°metros:**
- `file` (form-data, requerido): Archivo PDF, TXT o MD

**Ejemplo de uso con curl:**
```bash
curl -X POST "http://localhost:8000/upload" \
  -F "file=@documento.pdf"
```

**Ejemplo con Python:**
```python
import requests

url = "http://localhost:8000/upload"
files = {"file": open("documento.pdf", "rb")}
response = requests.post(url, files=files)
print(response.json())
```

**Respuesta exitosa:**
```json
{
  "doc_id": "doc_5b89c441ba45",
  "chunks_created": 120,
  "total_chars": 45678,
  "filename": "documento.pdf",
  "message": "Documento procesado exitosamente"
}
```

**Errores comunes:**
- `400`: Formato no soportado
- `500`: Error al procesar el documento

---

#### 3. Realizar Consulta RAG

Procesa una consulta y genera una respuesta usando RAG.

**Endpoint:** `POST /query`

**Content-Type:** `application/json`

**Body:**
```json
{
  "question": "¬øQu√© es Python?",
  "top_k": 5,
  "return_sources": true
}
```

**Par√°metros:**
- `question` (string, requerido): Pregunta del usuario
- `top_k` (int, opcional): N√∫mero de chunks a recuperar (default: 5)
- `return_sources` (bool, opcional): Si retornar fuentes (default: true)

**Ejemplo de uso con curl:**
```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "¬øQu√© es Python?",
    "top_k": 5,
    "return_sources": true
  }'
```

**Ejemplo con Python:**
```python
import requests

url = "http://localhost:8000/query"
payload = {
    "question": "¬øC√≥mo definir una funci√≥n en Python?",
    "top_k": 5,
    "return_sources": True
}
response = requests.post(url, json=payload)
result = response.json()

print("Respuesta:", result["answer"])
print("Fuentes:", result["sources"])
```

**Respuesta:**
```json
{
  "answer": "Python es un lenguaje de programaci√≥n de alto nivel...",
  "sources": [
    {
      "filename": "Python para todos.pdf",
      "chunk_index": 2,
      "score": 0.85
    },
    {
      "filename": "Introduccion a Python.pdf",
      "chunk_index": 5,
      "score": 0.78
    }
  ],
  "num_chunks": 5
}
```

**Proceso interno:**
1. Genera embedding de la pregunta
2. B√∫squeda sem√°ntica en ChromaDB
3. Recupera top-k chunks m√°s relevantes
4. Genera respuesta con GPT-4 usando contexto
5. Retorna respuesta con fuentes

---

#### 4. Listar Documentos

Obtiene la lista de documentos indexados.

**Endpoint:** `GET /documents`

**Ejemplo:**
```bash
curl http://localhost:8000/documents
```

**Respuesta:**
```json
{
  "documents": [
    "doc_5b89c441ba45",
    "doc_7c3d2e1f9a8b"
  ],
  "count": 2
}
```

---

#### 5. Estad√≠sticas

Obtiene estad√≠sticas de la base de datos.

**Endpoint:** `GET /stats`

**Ejemplo:**
```bash
curl http://localhost:8000/stats
```

**Respuesta:**
```json
{
  "total_chunks": 718,
  "unique_documents": 6
}
```

---

#### 6. M√©tricas del Sistema

Obtiene m√©tricas de rendimiento.

**Endpoint:** `GET /metrics`

**Ejemplo:**
```bash
curl http://localhost:8000/metrics
```

**Respuesta:**
```json
{
  "timestamp": "2025-11-04T18:00:00",
  "system_stats": {
    "total_chunks": 718,
    "unique_documents": 6,
    "db_size_mb": 19.52
  },
  "query_metrics": {
    "total_queries": 25,
    "time_metrics": {
      "total_time": {
        "mean": 2.5,
        "min": 1.2,
        "max": 5.8
      }
    }
  }
}
```

---

#### 7. Eliminar Documento

Elimina un documento y todos sus chunks.

**Endpoint:** `DELETE /delete/{doc_id}`

**Ejemplo:**
```bash
curl -X DELETE "http://localhost:8000/delete/doc_5b89c441ba45"
```

---

### Documentaci√≥n Interactiva

Accede a la documentaci√≥n interactiva (Swagger) en:
```
http://localhost:8000/docs
```

O la documentaci√≥n alternativa (ReDoc) en:
```
http://localhost:8000/redoc
```

### Ejemplo Completo: Flujo de Trabajo

```bash
# 1. Verificar estado
curl http://localhost:8000/health

# 2. Subir documento
curl -X POST "http://localhost:8000/upload" \
  -F "file=@python_tutorial.pdf"

# 3. Ver estad√≠sticas
curl http://localhost:8000/stats

# 4. Hacer consulta
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "¬øC√≥mo definir una funci√≥n en Python?",
    "top_k": 5
  }'

# 5. Ver m√©tricas
curl http://localhost:8000/metrics
```

## Stack Tecnol√≥gico

### Backend

| Tecnolog√≠a | Versi√≥n | Prop√≥sito |
|------------|---------|-----------|
| **Python** | 3.9+ | Lenguaje principal del sistema |
| **FastAPI** | 0.104+ | Framework web moderno y r√°pido para API REST |
| **Uvicorn** | 0.24+ | Servidor ASGI de alto rendimiento |
| **LangChain** | 0.1+ | Framework para aplicaciones LLM |
| **ChromaDB** | 0.4+ | Base de datos vectorial para embeddings |
| **Sentence-Transformers** | 2.2+ | Modelo de embeddings sem√°nticos |
| **OpenAI** | 1.0+ | SDK para integraci√≥n con GPT-4 |
| **PyPDF2** | 3.0+ | Extracci√≥n de texto de PDFs |
| **python-dotenv** | 1.0+ | Manejo de variables de entorno |

### Frontend

| Tecnolog√≠a | Versi√≥n | Prop√≥sito |
|------------|---------|-----------|
| **Gradio** | 4.0+ | Interfaz web interactiva para ML/AI |
| **HTML/CSS/JS** | - | Interfaz de usuario (generada por Gradio) |

### Infraestructura

| Tecnolog√≠a | Versi√≥n | Prop√≥sito |
|------------|---------|-----------|
| **Docker** | 20.10+ | Contenerizaci√≥n de aplicaciones |
| **Docker Compose** | 2.0+ | Orquestaci√≥n de servicios |

### Herramientas de Desarrollo

| Tecnolog√≠a | Prop√≥sito |
|------------|-----------|
| **Git** | Control de versiones |
| **pydantic** | Validaci√≥n de datos |
| **pytest** | Testing (opcional) |

## üèõÔ∏è Decisiones Arquitect√≥nicas

### 1. Base de Datos Vectorial: ChromaDB

**Decisi√≥n**: Usar ChromaDB como base de datos vectorial principal.

**Razones:**
- **Simplicidad**: F√°cil de instalar y configurar (no requiere servidor externo)
- **Persistencia local**: Almacena datos localmente sin dependencias externas
- **Rendimiento**: Excelente para bases peque√±as-medianas (< 100K vectores)
- **Metadata filtering**: Soporte nativo para filtrar por metadata
- **Open source**: C√≥digo abierto y activamente mantenido

**Alternativas consideradas:**
- **Milvus**: M√°s complejo, requiere servidor separado
- **Pinecone**: Servicio cloud, requiere suscripci√≥n
- **Weaviate**: M√°s pesado, overkill para este caso

**Trade-offs aceptados:**
- Limitado a bases de datos locales
- Rendimiento puede degradarse con > 1M vectores

### 2. Modelo de Embeddings: Sentence-Transformers (all-MiniLM-L6-v2)

**Decisi√≥n**: Usar modelo local de Sentence-Transformers.

**Razones:**
- **Sin dependencias externas**: No requiere API keys adicionales
- **Velocidad**: Modelo optimizado para rapidez (6 layers, 384 dimensions)
- **Calidad**: Buen balance calidad/velocidad para espa√±ol
- **Costo**: Gratis, sin costos por embedding
- **Privacidad**: Datos no salen del servidor

**Alternativas consideradas:**
- **OpenAI Embeddings**: Mayor calidad pero con costo y latencia
- **Cohere**: Similar a OpenAI, requiere API key
- **Modelos m√°s grandes**: Mejor calidad pero m√°s lentos

**Trade-offs aceptados:**
- Calidad ligeramente inferior a embeddings de OpenAI
- Requiere descarga inicial del modelo (~90MB)

### 3. LLM: OpenAI GPT-4-turbo

**Decisi√≥n**: Usar GPT-4-turbo como modelo de generaci√≥n.

**Razones:**
- **Calidad superior**: Mejor entendimiento de contexto y generaci√≥n
- **API estable**: Servicio confiable y bien documentado
- **Capacidades avanzadas**: Buen manejo de instrucciones complejas
- **Actualizaci√≥n autom√°tica**: Siempre √∫ltima versi√≥n sin reentrenamiento

**Alternativas consideradas:**
- **GPT-3.5-turbo**: M√°s r√°pido y barato, pero menor calidad
- **Anthropic Claude**: Similar calidad, pero menos integrado
- **Groq**: Muy r√°pido pero requiere modelo espec√≠fico
- **Modelos locales (Llama)**: Sin costo pero requiere GPU potente

**Trade-offs aceptados:**
- Costo por token (aproximadamente $0.01-0.03 por consulta)
- Dependencia de conexi√≥n a internet
- Latencia de red (~2-5 segundos por consulta)

### 4. Estrategia de Chunking

**Decisi√≥n**: Chunking por palabras con overlap.

**Configuraci√≥n:**
- `CHUNK_SIZE=500` palabras
- `CHUNK_OVERLAP=50` palabras

**Razones:**
- **Preserva contexto**: Overlap evita perder informaci√≥n en l√≠mites
- **Tama√±o √≥ptimo**: 500 palabras balancea contexto y precisi√≥n
- **Por palabras, no caracteres**: Mejor calidad sem√°ntica
- **Configurable**: F√°cil ajustar seg√∫n necesidad

**Alternativas consideradas:**
- **Chunking por caracteres**: M√°s simple pero peor calidad
- **Chunking por p√°rrafos**: M√°s natural pero tama√±o variable
- **Chunking inteligente (sem√°ntico)**: Mejor pero m√°s complejo

**Trade-offs aceptados:**
- Puede dividir conceptos entre chunks
- Overlap aumenta tama√±o de la BD

### 5. Framework Web: FastAPI

**Decisi√≥n**: Usar FastAPI para la API REST.

**Razones:**
- **Rendimiento**: Muy r√°pido (comparable a Node.js)
- **Documentaci√≥n autom√°tica**: Swagger/OpenAPI integrado
- **Type hints**: Validaci√≥n autom√°tica con Pydantic
- **Async/await**: Soporte nativo para operaciones as√≠ncronas
- **Moderno**: Dise√±o limpio y Pythonic

**Alternativas consideradas:**
- **Flask**: M√°s simple pero menos features
- **Django**: M√°s pesado, overkill para API
- **Express.js**: Requerir√≠a cambiar stack

### 6. Interfaz Web: Gradio

**Decisi√≥n**: Usar Gradio para la interfaz de usuario.

**Razones:**
- **R√°pido de desarrollar**: Interfaz lista en minutos
- **Interactivo**: Chat, uploads, visualizaci√≥n incluidos
- **Sin frontend**: No requiere HTML/CSS/JS manual
- **Integraci√≥n f√°cil**: Se conecta directamente a la API
- **Gratis y open source**: Sin restricciones

**Alternativas consideradas:**
- **Streamlit**: Similar pero menos flexible
- **React/Vue**: M√°s control pero mucho m√°s trabajo
- **HTML/CSS/JS puro**: M√°ximo control pero desarrollo largo

### 7. Arquitectura de Servicios

**Decisi√≥n**: Separar servicios en m√≥dulos independientes.

**Estructura:**
```
services/
‚îú‚îÄ‚îÄ document_processor/  # Procesamiento
‚îú‚îÄ‚îÄ rag_query/          # Consultas RAG
‚îú‚îÄ‚îÄ web_interface/      # API y UI
‚îî‚îÄ‚îÄ metrics/            # M√©tricas
```

**Razones:**
- **Separaci√≥n de responsabilidades**: Cada m√≥dulo tiene una funci√≥n clara
- **Reutilizable**: M√≥dulos pueden usarse independientemente
- **Testeable**: F√°cil testear cada componente
- **Escalable**: F√°cil agregar nuevos servicios

### 8. Manejo de Configuraci√≥n

**Decisi√≥n**: Sistema centralizado de configuraci√≥n con `config/settings.py`.

**Razones:**
- **Un solo punto de verdad**: Toda la configuraci√≥n en un lugar
- **Validaci√≥n autom√°tica**: Verifica configuraci√≥n al inicio
- **Type-safe**: Type hints para todas las variables
- **Seguridad**: Manejo seguro de secrets

## Estrategias de Evaluaci√≥n

### 1. M√©tricas Implementadas

#### M√©tricas de Recuperaci√≥n (Retrieval)

**Score de Similitud (Cosine Similarity)**
- **Qu√© mide**: Relevancia de chunks recuperados
- **Rango**: 0.0 - 1.0 (1.0 = perfectamente relevante)
- **Umbral recomendado**: > 0.5 para chunks √∫tiles
- **Implementaci√≥n**: Calculado autom√°ticamente en cada b√∫squeda

**Precision@K**
- **Qu√© mide**: Porcentaje de chunks relevantes en los top-K
- **C√°lculo**: `chunks_relevantes / K`
- **Objetivo**: > 70% para K=5

**Tiempo de Recuperaci√≥n**
- **Qu√© mide**: Velocidad de b√∫squeda sem√°ntica
- **Objetivo**: < 500ms para bases < 10K chunks
- **Implementado**: Tracking autom√°tico

#### M√©tricas de Generaci√≥n

**Tiempo de Generaci√≥n**
- **Qu√© mide**: Latencia del LLM
- **Depende de**: Modelo, contexto, longitud de respuesta
- **Objetivo**: 2-5 segundos con GPT-4

**Longitud de Respuesta**
- **Qu√© mide**: Completitud de la respuesta
- **An√°lisis**: Respuestas muy cortas (< 50 chars) pueden indicar falta de contexto

**Relevancia de Respuesta**
- **Qu√© mide**: Si la respuesta responde a la pregunta
- **Evaluaci√≥n**: Manual o con LLM evaluador
- **M√©trica**: 0-5 (subjetiva)

#### M√©tricas del Sistema

**Throughput**
- **Qu√© mide**: Consultas procesadas por segundo
- **C√°lculo**: `total_queries / total_time`
- **Objetivo**: 0.2-0.5 queries/segundo con GPT-4

**Tasa de √âxito**
- **Qu√© mide**: Porcentaje de consultas exitosas
- **Objetivo**: > 95%

### 2. Herramientas de Evaluaci√≥n

#### Script de M√©tricas

```bash
python scripts/generate_metrics_report.py
```

Genera reporte con:
- Tiempos promedio, m√≠nimos, m√°ximos
- Scores de similitud
- Estad√≠sticas de uso
- √öltimas 10 consultas

#### API de M√©tricas

```bash
curl http://localhost:8000/metrics
```

Retorna m√©tricas en tiempo real en formato JSON.

### 3. M√©todos de Evaluaci√≥n

#### Evaluaci√≥n Autom√°tica

**M√©tricas cuantitativas**:
- Tiempos de respuesta
- Scores de similitud
- Throughput
- Tasa de errores

**Implementaci√≥n**:
- Tracking autom√°tico en cada consulta
- Almacenamiento en `data/metrics.json`
- Reportes generados autom√°ticamente

#### Evaluaci√≥n Manual

**Checklist de calidad** (para cada respuesta):
- [ ] Relevancia (0-5): ¬øResponde a la pregunta?
- [ ] Precisi√≥n (0-5): ¬øLa informaci√≥n es correcta?
- [ ] Completitud (0-5): ¬øEst√° completa la respuesta?
- [ ] Coherencia (0-5): ¬øTiene sentido?

**Dataset de prueba**:
- Crear conjunto de preguntas con respuestas esperadas
- Comparar respuestas generadas vs esperadas
- Calcular m√©tricas de precisi√≥n/recall

### 4. Benchmarking

#### M√©tricas de Referencia

Para un sistema RAG con:
- Base de datos: 1000-5000 chunks
- Modelo: GPT-4-turbo
- Embeddings: Sentence-Transformers

**M√©tricas esperadas**:
- Tiempo de recuperaci√≥n: 200-500ms
- Tiempo de generaci√≥n: 2-5s
- Tiempo total: 2.5-6s
- Score promedio: 0.6-0.8
- Throughput: 0.15-0.4 queries/segundo

#### Comparaci√≥n con Baselines

**Baseline 1: Sin RAG (solo LLM)**
- Sin contexto de documentos
- Respuestas gen√©ricas
- No espec√≠ficas al dataset

**Baseline 2: RAG con b√∫squeda exacta**
- B√∫squeda por palabras clave
- Sin embeddings
- Menor precisi√≥n sem√°ntica

**Baseline 3: Diferente modelo de embeddings**
- Comparar Sentence-Transformers vs OpenAI embeddings
- Trade-off calidad/velocidad

### 5. Optimizaci√≥n Basada en M√©tricas

**Si score de similitud < 0.5**:
- Ajustar `CHUNK_SIZE`
- Cambiar modelo de embeddings
- Mejorar preprocesamiento

**Si tiempo de respuesta > 10s**:
- Reducir `TOP_K_RESULTS`
- Usar GPT-3.5 en lugar de GPT-4
- Optimizar b√∫squeda vectorial

**Si respuestas incompletas**:
- Aumentar `TOP_K_RESULTS`
- Aumentar `CHUNK_SIZE`
- Mejorar prompt del LLM

### 6. Monitoreo Continuo

**M√©tricas a monitorear**:
1. Latencia por consulta
2. Throughput (consultas/minuto)
3. Score promedio de similitud
4. Tasa de errores
5. Uso de tokens de OpenAI

**Alertas recomendadas**:
- Tiempo de respuesta > 10s
- Score promedio < 0.4
- Tasa de errores > 5%

Para m√°s detalles, ver [EVALUACION_RAG.md](EVALUACION_RAG.md)

## Uso del Sistema

### Flujo de Trabajo T√≠pico

1. **Subir Documentos**
   ```bash
   curl -X POST "http://localhost:8000/upload" \
     -F "file=@documento.pdf"
   ```

2. **Hacer Consultas**
   ```bash
   curl -X POST "http://localhost:8000/query" \
     -H "Content-Type: application/json" \
     -d '{"question": "¬øQu√© es Python?"}'
   ```

3. **Ver M√©tricas**
   ```bash
   python scripts/generate_metrics_report.py
   ```

### Desde la Interfaz Web

1. Abre http://localhost:7860
2. Sube un documento PDF/TXT/MD
3. Haz preguntas en el chat
4. El sistema responder√° con informaci√≥n de los documentos

## Soluci√≥n de Problemas

### Error: "OPENAI_API_KEY no est√° configurada"

**Soluci√≥n:**
1. Verifica que `.env` existe: `ls .env` o `dir .env`
2. Verifica contenido: `cat .env | grep OPENAI_API_KEY`
3. Debe contener: `OPENAI_API_KEY=sk-...`
4. Ejecuta: `python scripts/check_config.py`

### Error: "Base de datos vac√≠a"

**Soluci√≥n:**
1. Sube al menos un documento primero
2. Usa endpoint `/upload` o interfaz web
3. Verifica con: `curl http://localhost:8000/stats`

### Error: "Puerto en uso"

**Soluci√≥n:**
```bash
# Cambiar puerto en .env
API_PORT=8001
UI_PORT=7861
```

### Error: "Connection refused" en Docker

**Soluci√≥n:**
```bash
# Verificar contenedores
docker-compose ps

# Ver logs
docker-compose logs -f

# Reiniciar
docker-compose restart
```

### Error: "Module not found"

**Soluci√≥n:**
```bash
# Reinstalar dependencias
pip install -r requirements.txt

# Verificar entorno virtual activado
which python  # Linux/Mac
where python  # Windows
```

## Estructura del Proyecto

```
RAG-system-openai/
‚îú‚îÄ‚îÄ config/                      # Configuraci√≥n centralizada
‚îÇ   ‚îú‚îÄ‚îÄ settings.py              # Manejo de secrets y variables
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ services/                    # Servicios principales
‚îÇ   ‚îú‚îÄ‚îÄ document_processor/      # Procesamiento de documentos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ processor.py         # L√≥gica de chunking y embeddings
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ rag_query/               # Servicio de consultas RAG
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ query_service.py     # B√∫squeda sem√°ntica y generaci√≥n
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ web_interface/           # API y UI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api.py               # Endpoints FastAPI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gradio_ui.py         # Interfaz web
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ metrics/                 # Sistema de m√©tricas
‚îÇ       ‚îú‚îÄ‚îÄ metrics_collector.py # Colector de m√©tricas
‚îÇ       ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ scripts/                     # Scripts utilitarios
‚îÇ   ‚îú‚îÄ‚îÄ setup_env.py             # Configuraci√≥n interactiva
‚îÇ   ‚îú‚îÄ‚îÄ setup_env_auto.py        # Configuraci√≥n autom√°tica
‚îÇ   ‚îú‚îÄ‚îÄ check_config.py          # Verificar configuraci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ generate_metrics_report.py # Generar reportes
‚îÇ   ‚îî‚îÄ‚îÄ test_setup.py            # Verificar dependencias
‚îú‚îÄ‚îÄ docker/                      # Dockerfiles
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.api           # Imagen para API
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile.ui            # Imagen para UI
‚îú‚îÄ‚îÄ data/                        # Datos (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ chroma_db/              # Base de datos vectorial
‚îÇ   ‚îú‚îÄ‚îÄ uploaded_documents/      # Documentos subidos
‚îÇ   ‚îî‚îÄ‚îÄ metrics.json             # M√©tricas (gitignored)
‚îú‚îÄ‚îÄ docker-compose.yml           # Orquestaci√≥n Docker
‚îú‚îÄ‚îÄ requirements.txt             # Dependencias Python
‚îú‚îÄ‚îÄ env.example                  # Plantilla de configuraci√≥n
‚îú‚îÄ‚îÄ README.md                    # Este archivo
‚îú‚îÄ‚îÄ API_DOCUMENTATION.md         # Documentaci√≥n detallada de APIs
‚îú‚îÄ‚îÄ CONFIGURACION.md             # Configuraci√≥n de secrets
‚îú‚îÄ‚îÄ EVALUACION_RAG.md            # Estrategias de evaluaci√≥n
‚îî‚îÄ‚îÄ INICIAR_SISTEMA.md           # Gu√≠a de inicio
```

## Seguridad

- Archivo `.env` en `.gitignore` (no se sube al repositorio)
- Secrets nunca se exponen en logs
- Validaci√≥n de configuraci√≥n al inicio
- Manejo seguro de variables de entorno

Ver [CONFIGURACION.md](CONFIGURACION.md) para m√°s detalles.

## Documentaci√≥n Adicional

- **[API_DOCUMENTATION.md](API_DOCUMENTATION.md)**: Documentaci√≥n completa de todos los endpoints
- **[CONFIGURACION.md](CONFIGURACION.md)**: Configuraci√≥n detallada de secrets y variables
- **[EVALUACION_RAG.md](EVALUACION_RAG.md)**: Estrategias completas de evaluaci√≥n
- **[INICIAR_SISTEMA.md](INICIAR_SISTEMA.md)**: Gu√≠a paso a paso para iniciar
- **[EVALUACION_REQUERIMIENTOS.md](EVALUACION_REQUERIMIENTOS.md)**: Checklist de requerimientos

## Pr√≥ximas Mejoras

- [ ] Autenticaci√≥n de usuarios
- [ ] Soporte para m√°s formatos (DOCX, HTML)
- [ ] Cach√© de respuestas frecuentes
- [ ] Evaluaci√≥n autom√°tica con dataset de prueba
- [ ] Soporte para m√∫ltiples bases de datos vectoriales
- [ ] Dashboard de m√©tricas en tiempo real
- [ ] API de streaming para respuestas
- [ ] Filtrado avanzado por metadata


---

**Documentaci√≥n Interactiva**: http://localhost:8000/docs  
**Interfaz Web**: http://localhost:7860  
**Repositorio**: https://github.com/TU_USUARIO/rag-system-openai
